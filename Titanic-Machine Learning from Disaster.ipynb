{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add All the Models Libraries\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Common data processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from scipy import sparse\n",
    "\n",
    "#Accuracy Score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the data for feature engineering and later split it, just before applying Data Pipeline\n",
    "TrainFile = pd.read_csv(\"U:\\\\Titanic Dataset\\\\train.csv\") #read the data from the csv file.\n",
    "TestFile = pd.read_csv(\"U:\\\\Titanic Dataset\\\\test.csv\")\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "DataFile = TrainFile.append(TestFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainFile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestFile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.881138</td>\n",
       "      <td>33.295479</td>\n",
       "      <td>0.385027</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.413493</td>\n",
       "      <td>51.758668</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>378.020061</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>1.041658</td>\n",
       "      <td>0.486592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>982.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age         Fare        Parch  PassengerId       Pclass  \\\n",
       "count  1046.000000  1308.000000  1309.000000  1309.000000  1309.000000   \n",
       "mean     29.881138    33.295479     0.385027   655.000000     2.294882   \n",
       "std      14.413493    51.758668     0.865560   378.020061     0.837836   \n",
       "min       0.170000     0.000000     0.000000     1.000000     1.000000   \n",
       "25%      21.000000     7.895800     0.000000   328.000000     2.000000   \n",
       "50%      28.000000    14.454200     0.000000   655.000000     3.000000   \n",
       "75%      39.000000    31.275000     0.000000   982.000000     3.000000   \n",
       "max      80.000000   512.329200     9.000000  1309.000000     3.000000   \n",
       "\n",
       "             SibSp    Survived  \n",
       "count  1309.000000  891.000000  \n",
       "mean      0.498854    0.383838  \n",
       "std       1.041658    0.486592  \n",
       "min       0.000000    0.000000  \n",
       "25%       0.000000    0.000000  \n",
       "50%       0.000000    0.000000  \n",
       "75%       1.000000    1.000000  \n",
       "max       8.000000    1.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFile.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      "Age            1046 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Name           1309 non-null object\n",
      "Parch          1309 non-null int64\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Sex            1309 non-null object\n",
      "SibSp          1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Ticket         1309 non-null object\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 132.9+ KB\n"
     ]
    }
   ],
   "source": [
    "DataFile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Split the names to gt Mr. or Miss or Mrs.\n",
    "\n",
    "FirstName = DataFile[\"Name\"].str.split(\"[,.]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now strip the white spaces from the Salutation\n",
    "titles = [str.strip(name[1]) for name in FirstName.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFile[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the columns - that may not impact the analysis\n",
    "DataFile = DataFile.drop('Name',axis=1)\n",
    "DataFile = DataFile.drop('PassengerId',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now split the Tickets to get special cabins. As Tickets may hold some valuable insights\n",
    "\n",
    "DataFile['Ticket'] = DataFile['Ticket'].apply(lambda x: str(x)[0])\n",
    "\n",
    "#Replace All the number values with N\n",
    "\n",
    "DataFile['Ticket'].replace(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], 'N', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now take the first letter of the Cabin and Impute O for other cabins imputed\n",
    "\n",
    "DataFile[[\"Cabin\"]] = DataFile[[\"Cabin\"]].fillna(value=\"O\")\n",
    "DataFile[\"Cabin\"] = DataFile['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now first we replace the extra titles to Mr and Mrs\n",
    "\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "\n",
    "DataFile.replace({'Title': mapping}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the imputed value for FARE\n",
    "DataFile['Fare'].fillna(DataFile['Fare'].median(), inplace=True)\n",
    "\n",
    "#based on the median taken to check fare the imputed value of Embarkment to be \"S\"\n",
    "DataFile['Embarked'].fillna(\"S\", inplace=True)\n",
    "\n",
    "#impute the age based on Titles \n",
    "titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n",
    "for title in titles:\n",
    "    imputed_age = DataFile.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    DataFile.loc[(DataFile['Age'].isnull()) & (DataFile['Title'] == title), 'Age'] = imputed_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge SibSp and Parch into one\n",
    "DataFile[\"Family Size\"] = DataFile[\"SibSp\"] + DataFile[\"Parch\"] + 1\n",
    "\n",
    "#Make Family Bins\n",
    "DataFile[\"Family Size\"] = pd.cut(DataFile[\"Family Size\"], bins=[0,1,4,20], labels=[1,2,3])\n",
    "\n",
    "#drop SibSp and Parch\n",
    "DataFile = DataFile.drop('SibSp',axis=1)\n",
    "DataFile = DataFile.drop('Parch',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making Fare bins\n",
    "\n",
    "DataFile['FareBin'] = pd.qcut(DataFile['Fare'], 5)\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataFile['FareBin'] = label.fit_transform(DataFile['FareBin'])\n",
    "DataFile = DataFile.drop('Fare',axis=1)\n",
    "\n",
    "#Making Age Bins\n",
    "DataFile['AgeBin'] = pd.qcut(DataFile['Age'], 4)\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataFile['AgeBin'] = label.fit_transform(DataFile['AgeBin'])\n",
    "DataFile = DataFile.drop('Age',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a dummy for male and female\n",
    "DataFile['Sex'].replace(['male','female'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family Size</th>\n",
       "      <th>FareBin</th>\n",
       "      <th>AgeBin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>P</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>G</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>P</td>\n",
       "      <td>Miss</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>D</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>O</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>C</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Miss</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>O</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cabin Embarked  Pclass  Sex  Survived Ticket   Title Family Size  FareBin  \\\n",
       "0       O        S       3    0       0.0      A      Mr           2        0   \n",
       "1       C        C       1    1       1.0      P     Mrs           2        4   \n",
       "2       O        S       3    1       1.0      S    Miss           1        1   \n",
       "3       C        S       1    1       1.0      N     Mrs           2        4   \n",
       "4       O        S       3    0       0.0      N      Mr           1        1   \n",
       "5       O        Q       3    0       0.0      N      Mr           1        1   \n",
       "6       E        S       1    0       0.0      N      Mr           1        4   \n",
       "7       O        S       3    0       0.0      N  Master           3        2   \n",
       "8       O        S       3    1       1.0      N     Mrs           2        2   \n",
       "9       O        C       2    1       1.0      N     Mrs           2        3   \n",
       "10      G        S       3    1       1.0      P    Miss           2        2   \n",
       "11      C        S       1    1       1.0      N    Miss           1        3   \n",
       "12      O        S       3    0       0.0      A      Mr           1        1   \n",
       "13      O        S       3    0       0.0      N      Mr           3        3   \n",
       "14      O        S       3    1       0.0      N    Miss           1        0   \n",
       "15      O        S       2    1       1.0      N     Mrs           1        2   \n",
       "16      O        Q       3    0       0.0      N  Master           3        3   \n",
       "17      O        S       2    0       1.0      N      Mr           1        2   \n",
       "18      O        S       3    1       0.0      N     Mrs           2        2   \n",
       "19      O        C       3    1       1.0      N     Mrs           1        0   \n",
       "20      O        S       2    0       0.0      N      Mr           1        3   \n",
       "21      D        S       2    0       1.0      N      Mr           1        2   \n",
       "22      O        Q       3    1       1.0      N    Miss           1        1   \n",
       "23      A        S       1    0       1.0      N      Mr           1        3   \n",
       "24      O        S       3    1       0.0      N    Miss           3        2   \n",
       "25      O        S       3    1       1.0      N     Mrs           3        3   \n",
       "26      O        C       3    0       0.0      N      Mr           1        0   \n",
       "27      C        S       1    0       0.0      N      Mr           3        4   \n",
       "28      O        Q       3    1       1.0      N    Miss           1        1   \n",
       "29      O        S       3    0       0.0      N      Mr           1        1   \n",
       "..    ...      ...     ...  ...       ...    ...     ...         ...      ...   \n",
       "388     O        Q       3    0       NaN      N      Mr           1        0   \n",
       "389     O        S       3    0       NaN      N  Master           3        2   \n",
       "390     B        S       1    0       NaN      N      Mr           1        4   \n",
       "391     D        S       1    1       NaN      P     Mrs           2        3   \n",
       "392     O        S       3    0       NaN      C  Master           2        2   \n",
       "393     O        S       2    0       NaN      C      Mr           1        1   \n",
       "394     O        S       3    0       NaN      N      Mr           3        3   \n",
       "395     C        S       1    1       NaN      N     Mrs           2        4   \n",
       "396     O        Q       3    0       NaN      N      Mr           1        0   \n",
       "397     B        C       1    1       NaN      N     Mrs           2        4   \n",
       "398     O        S       3    0       NaN      N      Mr           1        0   \n",
       "399     O        Q       3    0       NaN      N      Mr           1        0   \n",
       "400     C        S       1    1       NaN      N    Miss           1        4   \n",
       "401     O        S       2    0       NaN      N      Mr           2        2   \n",
       "402     O        C       1    1       NaN      N    Miss           2        4   \n",
       "403     O        S       1    0       NaN      N      Mr           1        4   \n",
       "404     D        C       1    0       NaN      N      Mr           2        3   \n",
       "405     D        C       2    0       NaN      S      Mr           1        2   \n",
       "406     O        S       2    0       NaN      N      Mr           2        1   \n",
       "407     C        C       1    0       NaN      N      Mr           2        4   \n",
       "408     O        Q       3    1       NaN      N    Miss           1        0   \n",
       "409     O        S       3    1       NaN      S    Miss           2        2   \n",
       "410     O        Q       3    1       NaN      N    Miss           1        0   \n",
       "411     C        Q       1    1       NaN      N     Mrs           2        4   \n",
       "412     O        S       3    1       NaN      N    Miss           1        0   \n",
       "413     O        S       3    0       NaN      A      Mr           1        1   \n",
       "414     C        C       1    1       NaN      P     Mrs           1        4   \n",
       "415     O        S       3    0       NaN      S      Mr           1        0   \n",
       "416     O        S       3    0       NaN      N      Mr           1        1   \n",
       "417     O        C       3    0       NaN      N  Master           2        3   \n",
       "\n",
       "     AgeBin  \n",
       "0         0  \n",
       "1         3  \n",
       "2         1  \n",
       "3         2  \n",
       "4         2  \n",
       "5         1  \n",
       "6         3  \n",
       "7         0  \n",
       "8         1  \n",
       "9         0  \n",
       "10        0  \n",
       "11        3  \n",
       "12        0  \n",
       "13        3  \n",
       "14        0  \n",
       "15        3  \n",
       "16        0  \n",
       "17        1  \n",
       "18        2  \n",
       "19        2  \n",
       "20        2  \n",
       "21        2  \n",
       "22        0  \n",
       "23        1  \n",
       "24        0  \n",
       "25        3  \n",
       "26        1  \n",
       "27        0  \n",
       "28        0  \n",
       "29        1  \n",
       "..      ...  \n",
       "388       0  \n",
       "389       0  \n",
       "390       1  \n",
       "391       3  \n",
       "392       0  \n",
       "393       3  \n",
       "394       1  \n",
       "395       0  \n",
       "396       1  \n",
       "397       3  \n",
       "398       0  \n",
       "399       2  \n",
       "400       1  \n",
       "401       3  \n",
       "402       0  \n",
       "403       0  \n",
       "404       3  \n",
       "405       0  \n",
       "406       1  \n",
       "407       3  \n",
       "408       0  \n",
       "409       0  \n",
       "410       0  \n",
       "411       3  \n",
       "412       1  \n",
       "413       1  \n",
       "414       3  \n",
       "415       3  \n",
       "416       1  \n",
       "417       0  \n",
       "\n",
       "[1309 rows x 10 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the file \n",
    "DataFile\n",
    "#DataFile.to_csv(\"Datafile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now split Back The data to training and test set - before applying the pipeline\n",
    "\n",
    "train_set, test_set = train_test_split(DataFile, test_size=0.3193,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 10)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape # This exactly matches the original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 10)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape # This exactly matches the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Observations</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgeBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FareBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family Size</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Number of Observations  Percent\n",
       "AgeBin                            0      0.0\n",
       "FareBin                           0      0.0\n",
       "Family Size                       0      0.0\n",
       "Title                             0      0.0\n",
       "Ticket                            0      0.0\n",
       "Survived                          0      0.0\n",
       "Sex                               0      0.0\n",
       "Pclass                            0      0.0\n",
       "Embarked                          0      0.0\n",
       "Cabin                             0      0.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n",
    "\n",
    "obs = train_set.isnull().sum().sort_values(ascending = False)\n",
    "percent = round(train_set.isnull().sum().sort_values(ascending = False)/len(train_set)*100, 2)\n",
    "pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Observations</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>418</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgeBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FareBin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family Size</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Number of Observations  Percent\n",
       "Survived                        418    100.0\n",
       "AgeBin                            0      0.0\n",
       "FareBin                           0      0.0\n",
       "Family Size                       0      0.0\n",
       "Title                             0      0.0\n",
       "Ticket                            0      0.0\n",
       "Sex                               0      0.0\n",
       "Pclass                            0      0.0\n",
       "Embarked                          0      0.0\n",
       "Cabin                             0      0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n",
    "\n",
    "obs = test_set.isnull().sum().sort_values(ascending = False)\n",
    "percent = round(test_set.isnull().sum().sort_values(ascending = False)/len(test_set)*100, 2)\n",
    "pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now define x and y.\n",
    "\n",
    "#the Y Variable\n",
    "train_set_y = train_set[\"Survived\"].copy()\n",
    "test_set_y = test_set[\"Survived\"].copy()\n",
    "\n",
    "#the X variables\n",
    "train_set_X = train_set.drop(\"Survived\", axis=1)\n",
    "test_set_X = test_set.drop(\"Survived\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here Starts the Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector([\"Ticket\",\"Title\",\"Cabin\",\"Embarked\"])),\n",
    "        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n",
    "    ])\n",
    "\n",
    "# num_pipeline = Pipeline([\n",
    "#        (\"selector\", DataFrameSelector([])),\n",
    "#        ('std_scaler', StandardScaler()),\n",
    "#      ])\n",
    "\n",
    "no_pipeline = Pipeline([\n",
    "        (\"selector\", DataFrameSelector([\"Sex\",\"Pclass\",\"FareBin\", \"AgeBin\",\"Family Size\"]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "    #(\"num_pipeline\", num_pipeline),\n",
    "    (\"no_pipeline\", no_pipeline),\n",
    "    ])\n",
    "\n",
    "final_train_X = full_pipeline.fit_transform(train_set_X)\n",
    "final_test_X = full_pipeline.transform(test_set_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now We Build the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 130 candidates, totalling 520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 520 out of 520 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28], 'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': [1, 6, 11, 16, 21]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Introduce KNN Classifier \n",
    "\n",
    "KNeighbours = KNeighborsClassifier()\n",
    "leaf_size = list(range(1,25,5))\n",
    "n_neighbors = list(range(4,30,2))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors,\n",
    "'algorithm' : ['auto'],\n",
    "'weights' : ['uniform', 'distance'],\n",
    "'leaf_size':leaf_size}\n",
    "\n",
    "grid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84736251402918072"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_grid = grid_search_KNeighbours.best_estimator_\n",
    "\n",
    "y_pred_neighbor_grid = neighbor_grid.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_neighbor_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_neigh_rand = neighbor_grid.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test1 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test1[\"PassengerId\"] = passenger_id_test\n",
    "result_test1[\"Survived\"] = y_pred_neigh_rand\n",
    "\n",
    "# Export the predictions file\n",
    "result_test1.to_csv(\"Titanic_prediction_ashish_KNN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another KNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1260 candidates, totalling 5040 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 498 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1310 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3902 tasks      | elapsed:   44.4s\n",
      "[Parallel(n_jobs=-1)]: Done 5040 out of 5040 | elapsed:   56.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighbours2 = KNeighborsClassifier()\n",
    "leaf_size2 = list(range(8,50,1))\n",
    "n_neighbors2 = list(range(5,20,1))\n",
    "\n",
    "param_grid_KNeighbours = {'n_neighbors' : n_neighbors2,\n",
    "'algorithm' : ['auto'],\n",
    "'weights' : ['uniform', 'distance'],\n",
    "'leaf_size':leaf_size2}\n",
    "\n",
    "grid_search_KNeighbours2 = GridSearchCV(KNeighbours2, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search_KNeighbours2.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84848484848484851"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_grid2 = grid_search_KNeighbours2.best_estimator_\n",
    "\n",
    "y_pred_neighbor_grid2 = neighbor_grid2.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_neighbor_grid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'oob_score': [True, False], 'min_samples_leaf': [1, 5, 10], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20] \n",
    "oob_score = [True, False]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10] \n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n",
    "                     'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84848484848484851"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_estimator = rand_search_forest.best_estimator_\n",
    "\n",
    "y_pred_random_estimator = random_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_random_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_forest_rand = random_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test2 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test2[\"PassengerId\"] = passenger_id_test\n",
    "result_test2[\"Survived\"] = y_pred_forest_rand\n",
    "\n",
    "# Export the predictions file\n",
    "result_test2.to_csv(\"Titanic_prediction_ashish_RF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   14.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.001, 0.01, 0.05, 0.09], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.09]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83277216610549942"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_estimator = rand_search_ada.best_estimator_\n",
    "\n",
    "y_pred_ada_estimator = ada_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_ada_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_ada_rand = ada_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test3 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test3[\"PassengerId\"] = passenger_id_test\n",
    "result_test3[\"Survived\"] = y_pred_ada_rand\n",
    "\n",
    "# Export the predictions file\n",
    "result_test3.to_csv(\"Titanic_prediction_ashish_ada.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'max_features': [5, 7, 10], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_classifier = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "max_features = [5, 7, 10]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]  # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88776655443322106"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_estimator = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "y_pred_extra_estimator = extra_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_extra_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   32.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000C3A1F98>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000000D13B4E0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_Classifier = SVC(random_state = 42)\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.0001, 0.001), \"C\": uniform(100000, 1000000)}\n",
    "\n",
    "rand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = -1)\n",
    "\n",
    "rand_search_svc.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87429854096520765"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_estimator = rand_search_svc.best_estimator_\n",
    "\n",
    "y_pred_svc_estimator = svc_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_svc_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_svc_rand = svc_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test4 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test4[\"PassengerId\"] = passenger_id_test\n",
    "result_test4[\"Survived\"] = y_pred_svc_rand \n",
    "\n",
    "# Export the predictions file\n",
    "result_test4.to_csv(\"Titanic_prediction_ashish_svc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 100, 400, 700, 1000], 'learning_rate': [0.1, 0.5], 'max_depth': [10, 20], 'min_samples_split': [2, 4, 10, 12, 16], 'min_samples_leaf': [1, 5, 10], 'max_features': [5, 20], 'max_leaf_nodes': [2, 10, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_Classifier = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [50, 100, 400, 700, 1000]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [10, 20]\n",
    "min_samples_split = [2, 4, 10, 12, 16]\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "max_features = [5, 20]\n",
    "max_leaf_nodes = [2, 10, 20]\n",
    "                            \n",
    "param_grid_grad_boost_class = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n",
    "                              'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "rand_search_grad_boost_class = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost_class, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost_class.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83950617283950613"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_estimator = rand_search_grad_boost_class.best_estimator_\n",
    "\n",
    "y_pred_gb_estimator = gb_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_gb_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_gb_rand = gb_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test5 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test5[\"PassengerId\"] = passenger_id_test\n",
    "result_test5[\"Survived\"] = y_pred_gb_rand \n",
    "\n",
    "# Export the predictions file\n",
    "result_test5.to_csv(\"Titanic_prediction_ashish_gb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  40 | elapsed:    5.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': array([ 0.1,  0.2, ...,  9.8,  9.9])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = np.array(list(range(1, 100)))/10\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84062850729517391"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_estimator = rand_search_log_reg.best_estimator_\n",
    "\n",
    "y_pred_log_estimator = log_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_log_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_log_rand = log_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test6 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test6[\"PassengerId\"] = passenger_id_test\n",
    "result_test6[\"Survived\"] = y_pred_log_rand \n",
    "\n",
    "# Export the predictions file\n",
    "result_test6.to_csv(\"Titanic_prediction_ashish_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Classifier - Nueral Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'alpha': [0.0001, 0.001, 0.01, 1], 'learning_rate_init': [0.0001, 0.001, 0.01, 1], 'max_iter': [50, 70, 100, 200], 'tol': [0.0001, 0.001, 0.01, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(random_state = 42)\n",
    "\n",
    "alpha = [.0001,.001,.01,1]\n",
    "learning_rate_init= [.0001,.001,.01,1]\n",
    "max_iter = [50,70,100,200]\n",
    "tol = [.0001,.001,.01,1]\n",
    "\n",
    "param_grid_mlp_clf = {'alpha':alpha, 'learning_rate_init':learning_rate_init, 'max_iter':max_iter,'tol':tol}\n",
    "\n",
    "rand_search_mlp_clf = RandomizedSearchCV(mlp_clf, param_grid_mlp_clf, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_mlp_clf.fit(final_train_X, train_set_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85297418630751964"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_estimator = rand_search_mlp_clf.best_estimator_\n",
    "\n",
    "y_pred_mlp_estimator = mlp_estimator.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_mlp_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get the predictions\n",
    "y_pred_mlp_rand = mlp_estimator.predict(final_test_X)\n",
    "\n",
    "# Prepare the predictions file\n",
    "result_test7 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test7[\"PassengerId\"] = passenger_id_test\n",
    "result_test7[\"Survived\"] = y_pred_mlp_rand \n",
    "\n",
    "# Export the predictions file\n",
    "result_test7.to_csv(\"Titanic_prediction_ashish_mlp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting Classifier - Ensemble the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=2.7000000000000002, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)), (...estimators=50, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_estimator),('rf',random_estimator), ('ada',ada_estimator),('gb', gb_estimator), ('knn', neighbor_grid),\n",
    "                ('svc', svc_estimator), ('mlp', mlp_estimator), ('ext', extra_estimator)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.840628507295\n",
      "RandomForestClassifier 0.848484848485\n",
      "AdaBoostClassifier 0.832772166105\n",
      "GradientBoostingClassifier 0.83950617284\n",
      "KNeighborsClassifier 0.847362514029\n",
      "SVC 0.874298540965\n",
      "MLPClassifier 0.852974186308\n",
      "ExtraTreesClassifier 0.887766554433\n",
      "VotingClassifier 0.86531986532\n"
     ]
    }
   ],
   "source": [
    "#Ensemble out the best models to get better accuracy score.\n",
    "\n",
    "for clf in (log_estimator, random_estimator, ada_estimator, gb_estimator, neighbor_grid, svc_estimator, mlp_estimator, extra_estimator, voting_clf):\n",
    "    clf.fit(final_train_X, train_set_y)\n",
    "    y_pred = clf.predict(final_train_X)\n",
    "    print(clf.__class__.__name__, accuracy_score(train_set_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced Voting Classifier - Remove Gradient Boosting and ADA Estimators- This doesn't improve performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_estimators = [\n",
    "    (\"log_reg_clf\", log_estimator),\n",
    "    (\"rnd_clf\", random_estimator),\n",
    "    (\"extra_clf\", extra_estimator),\n",
    "    (\"mlp_clf\", mlp_estimator),\n",
    "    (\"knn_clf\", neighbor_grid),\n",
    "    ('svc_clf', svc_estimator)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voting_clf2 = VotingClassifier(total_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log_reg_clf', LogisticRegression(C=2.7000000000000002, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=F...rbf', max_iter=-1, probability=False, random_state=42,\n",
       "  shrinking=True, tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf2.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86644219977553316"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the y_pred to get accuracy score.\n",
    "y_pred_voting2 = voting_clf2.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_voting2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Ensemble Technique : Bagging Model - Not the best model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            ...n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [50, 70, 100, 200, 500], 'max_samples': [10, 50, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "n_estimators = [50,70,100,200,500]\n",
    "max_samples = [10,50,100]\n",
    "\n",
    "param_grid_bag_clf = {'n_estimators':n_estimators, 'max_samples':max_samples}\n",
    "\n",
    "rand_search_bag_clf = RandomizedSearchCV(Bag_Classifier, param_grid_bag_clf, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_bag_clf.fit(final_train_X, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86083052749719413"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the y_pred to get accuracy score.\n",
    "y_pred_bag = rand_search_bag_clf.predict(final_train_X)\n",
    "accuracy_score(train_set_y, y_pred_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC looks like a clear winner with 87% accuracy on training set. But Kaggle predicts Voting Classifier as the best classification algorithm on Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict using voting\n",
    "y_pred_voting = voting_clf.predict(final_test_X)\n",
    "\n",
    "#predict using voting 2nd version.\n",
    "y_pred_voting2 = voting_clf2.predict(final_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the datafile for voting classifier 2\n",
    "result_test8 = pd.DataFrame()\n",
    "passenger_id_test = TestFile[\"PassengerId\"].copy()\n",
    "result_test8[\"PassengerId\"] = passenger_id_test\n",
    "result_test8[\"Survived\"] = y_pred_voting2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Voting Classifier results\n",
    "result_test8.to_csv(\"Titanic_prediction_ashish_Vot.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
